Solr
ElasticSearch
Hibernate Search


Advantages of using ElasticSearch over Lucene
(1) Using pure Lucene is challenging as it is a library.
(2) Not have distributed support
(3) ElasticSearch Supports Http and JSON
(4) ElasticSearch will work in Distributed and sharded environment

https://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage

=====================Chapter 1=====================

Analysing Raw Content
	-> words are converted to tokens
	-> these tokens can be similar in sound,grammer,singular/plural
	-> this process of creating tokens is done by Analyser
	-> Analysers can also be applied in a chain, one after the other

Searching in index
	->precision and recall
	->single and multiterm queries
	->phrase queries, fuzzy queries, wildcards
	->result ranking, sorting
	
QueryParser is used to make queries in lucene search
We can also boost some fields while making a query

3 models for Search

1) Pure Boolean model : either it will match fully or not.
						no score is associated with document
2) Vector Space model

3) Probabilistic model : probability is computed for a document to match with query.

Admin Configurations:
-ram buffer size
-how many segments to merge at once
-how often commit changes
-how often to delete unused indexes

lucene do not provide scaling
solr provides both sharding and replication

--------------------------

Document (Field)   -->  Analyser  -->  IndexWriter  -->  Directory

Document - Collection of fields
Analyser  -> stop words, lower/upper case, 


Lucene understands -> String and numeric values like int and float

Search classes :
	IndexSearcher
	Term
	Query
		TermQuery, BooleanQuery, PhraseQuery, TermRangeQuery, NumericRangeQuery
		setBoost()
	TopDocs
	
=====================Chapter 2=====================

A field must be indexed if you want to search on it.
if indexed , a field may also contain term vector
even if not indexed, a field may be stored in index and not used in search.

When we get a document, only stored fields are fetched, indexed and not stored fields are not fetched.

->Schema less design
->multiple entities can be stored in single document
->two same documents can have different schema(one is old and one is new)
->flat structure , denormalized data

=>Indexing Process

raw text taken from source
	|
	|
text is analysed by chain of analysers and converted into stream of tokens
	|
	|
tokens are stored as index


Analysis
	->Analysers
	->Filter
		->stop filter, lowercase filter,stem filter
		

Inverted Index structure
	->tokens are treated as keys for document
	->key=token and value=document
	->which document contain X
	
	
addDocument(Document)
	->takes default analser provided by IndexWriter
addDocument(Document, Analyser)

NOTE : make sure that analyser provided during indexing must match analser during search.

Analyser
	WhiteSpaceAnalyser

Index
	NO
	ANALYZED		--> analyser is used to break the content into tokens
	NOT_ANALYZED	--> analser is not used, content is used as it is, 
						used for those data that should not be broken
	NOT_ANALYZED_NO_NORMS
	ANALYZED_NO_NORMS
	
Store
	NO
	YES
	
TermVector
	NO
	YES
	WITH_POSITIONS
	WITH_OFFSETS
	WITH_POSITIONS_OFFSETS
	

---------------------
	
Term Vector

A term vector is a list of the document's terms and their number of occurrences in that document.
A term is the basic unit searchable in Lucene

Term t = new Term( "field" , "TermText");

when you search Lucene index, you are searching terms
If a field of the document enabled term vector, all terms in that field will be added to document's term vector.

Index
	key --> document id
	
Term Vector 
	term --> (frequency, position, offset)
	
Document Term Vector contains
	->the document id
	->the field name
	->the text of term
	->the frequency
	->position and offsets
	
Applications of Term vector :
-> search result highlighting
-> "related posts" feature in a blog entry

Disadvantage :
->They store a lot of information and often take up a lot of disk space
-> make indexing and searching slower

http://makble.com/what-is-term-vector-in-lucene
http://blog.jpountz.net/post/41301889664/putting-term-vectors-on-a-diet

------------------------

Field options for sorting :

While fetching documents , documents are sorted according to their score by default.
To make these results, sorted by a column , we need to index that column.


multi values fields :

Document doc = new Document();
for (String author : authors) {
	doc.add(new Field("author", author,
	Field.Store.YES,
	Field.Index.ANALYZED));
}

=>Boosting
Boosting may be done during indexing or during searching.

Search-time boosting is more dynamic , but consumes more CPU
 allows you to expose the choice to the user
do not perform much boosting

document.setBoost(1.5F); 
default factor is 1.0 

Field subjectField = new Field("subject", subject, Field.Store.YES, Field.Index.ANALYZED);
subjectField.setBoost(1.2F);


Norms :

All boosts of document and field are combined and then compactly encoded (quantized) 
into a single byte, which is stored per field per document. 
During searching, norms for any field being searched are loaded into memory,
 decoded back into a floating-point number, and
used when computing the relevance score.

Norms for lucene index can be disabled.


=>Indexing Numbers dates and time
(1)
when you want to include numbers as string tokens : WhitespaceAnalyzer and StandardAnalyzer 
when you want to exclude numbers from text while making tokens : SimpleAnalyzer and StopAnalyzer

eg : “Be sure to include Form 1099 in your tax return”

(2)
Numbers where you want precise (equals) matching, range searching, and/or sorting
adding double value to document :
	doc.add(new NumericField("price").setDoubleValue(19.99));

Each numeric value is indexed using a trie structure.
NumericField can also easily handle dates and times by converting them to equivalent ints or longs.

adding timestamp to document :
	doc.add(new NumericField("timestamp").setLongValue(new Date().getTime()));
adding date to document : 
	doc.add(new NumericField("day").setIntValue((int) (new Date().getTime()/24/3600))); 

	
=> Field Truncation :

IndexWriter allows you to truncate per-Field
indexing so that only the first N terms are indexed for an analyzed field.
 text beyond the Nth term is completely ignored

MaxFieldLength.UNLIMITED
MaxFieldLength.LIMITED

=> Directory classes :

SimpleFSDirectory
NIOFSDirectory
MMapDirectory
RAMDirectory
FileSwitchDirectory



=> Concurrency, thread safety, and locking issues

-> accessing an index from multiple JVMs,
-> thread safety of IndexReader and IndexWriter, and 
-> the locking mechanism that Lucene uses to enforce these rules


(1) Any number of read-only IndexReaders may be open at once on a single index.
within a single JVM it’s best for resource utilization 
 and performance reasons to share a single IndexReader instance

(2) Only a single writer may be open on an index at once.
As soon as an IndexWriter is created, a write lock is obtained. 
Only when that IndexWriter is closed is the write lock released

(3) When IndexWriter is open for an index, 
 IndexReader will read document data at the state which was just before taking lock.
 
(4) All threads should share single instance of IndexReader and IndexWriter.
These classes are :
	Thread safe
	Thread friendly
	

=> Buffering and flushing

Documents are initially buffered in memory instead of being immediately written to the disk.
This buffering is done for performance reasons to minimize disk I/O. 
Periodically, these changes are flushed to the index Directory as a new segment.



=====================Chapter 3=====================

 When you’re querying a Lucene index, a TopDocs instance, containing an ordered
array of ScoreDoc, is returned. The array is ordered by score by default

The ScoreDocs themselves aren’t the actual matching documents, but rather references, via
an integer document ID, to the documents matched.


=>programmatically constructing your query

	Term t = new Term("subject", "ant");
	Query query = new TermQuery(t);
	TopDocs docs = searcher.search(query, 10);


=>using QueryParser to translate text entered by the user 

	QueryParser parser = new QueryParser(Version.LUCENE_30, "contents", new SimpleAnalyzer());
	Query query = parser.parse("+JUNIT +ANT -MOCK");
	TopDocs docs = searcher.search(query, 10);


Expression examples that QueryParser handles :

(1)
java			Contain the term java in the default field

(2)
java junit
java OR junit
				Contain the term java or junit, or both, in the default fielda
(3)
+java +junit
java AND junit
				Contain both java and junit in the default field

(4)				
title:ant 		Contain the term ant in the title field

(5)
title:extreme
–subject:sports

title:extreme
AND NOT subject:sports
				Contain extreme in the title field and don’t have sports in the subject field
					
(6)
(agile OR extreme) AND
methodology
				Contain methodology and must also contain agile and/or extreme, all in the default field

(7)
title:"junit in action"
				Contain the exact phrase “junit in action” in the title field
					
(8)					
java* 			Contain terms that begin with java, like javaspaces, javaserver, java.net, and the exact tem java itself.

(9)
java~ 			Contain terms that are close to the word java, such as lava


----------------

**QueryParser requires an analyzer to break pieces of the query text into terms.
QueryParser is the only searching piece that uses an analyzer, 
other searching APIs do not require analser.
They analyse on the basis of data provided in the API.


Directory dir = FSDirectory.open(new File("/path/to/index"));
IndexReader reader = IndexReader.open(dir);
IndexSearcher searcher = new IndexSearcher(reader);


=> Using explain() to understand hit scoring
	
	TopDocs topDocs = searcher.search(query, 10);
	for (ScoreDoc match : topDocs.scoreDocs) {
		Explanation explanation = searcher.explain(query, match.doc);
	}
	


=> Lucene’s diverse queries

TermQuery,TermRangeQuery,NumericRangeQuery,PrefixQuery,BooleanQuery,PhraseQuery,WildcardQuery,FuzzyQuery

(1)
Term t = new Term("isbn", "9781935182023");
Query query = new TermQuery(t);

(2)
//searching for all books whose title begins with any letter from d to j
TermRangeQuery query = new TermRangeQuery("title2", "d", "j", true, true);

(3)
//  201,003  == march 2010
NumericRangeQuery query = NumericRangeQuery.newIntRange("pubmonth", 200605, 200609, true, true);


(4)
Term term = new Term("category", "/technology/computers/programming"); 
PrefixQuery query = new PrefixQuery(term);


(5)
BooleanQuery
	->BooleanClause.Occur.MUST		-- only that matching documents are considered
	->BooleanClause.Occur.SHOULD	-- term is optional
	->BooleanClause.Occur.MUST_NOT 	--  any documents matching this clause are excluded
	

	TermQuery searchingBooks = new TermQuery(new Term("subject", "search"));
	Query books2010 = NumericRangeQuery.newIntRange("pubmonth", 201001, 201012, true, true);
	
	BooleanQuery searchingBooks2010 = new BooleanQuery();
	searchingBooks2010.add(searchingBooks, BooleanClause.Occur.MUST);
	searchingBooks2010.add(books2010, BooleanClause.Occur.MUST);
	
restricted to a maximum number of clauses : 1,024
	
(6)
PhraseQuery 
 It uses this information to locate documents
  where terms are within a certain distance of one another

The maximum allowable positional distance between terms to be considered a match is called slop. 
Distance is the number of positional moves of terms used to reconstruct the phrase in order.

