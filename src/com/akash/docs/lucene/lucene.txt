Solr
ElasticSearch
Hibernate Search


Advantages of using ElasticSearch over Lucene
(1) Using pure Lucene is challenging as it is a library.
(2) Not have distributed support
(3) ElasticSearch Supports Http and JSON
(4) ElasticSearch will work in Distributed and sharded environment

https://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage

=====================Chapter 1=====================

Analysing Raw Content
	-> words are converted to tokens
	-> these tokens can be similar in sound,grammer,singular/plural
	-> this process of creating tokens is done by Analyser
	-> Analysers can also be applied in a chain, one after the other

Searching in index
	->precision and recall
	->single and multiterm queries
	->phrase queries, fuzzy queries, wildcards
	->result ranking, sorting
	
QueryParser is used to make queries in lucene search
We can also boost some fields while making a query

3 models for Search

1) Pure Boolean model : either it will match fully or not.
						no score is associated with document
2) Vector Space model

3) Probabilistic model : probability is computed for a document to match with query.

Admin Configurations:
-ram buffer size
-how many segments to merge at once
-how often commit changes
-how often to delete unused indexes

lucene do not provide scaling
solr provides both sharding and replication

--------------------------

Document (Field)   -->  Analyser  -->  IndexWriter  -->  Directory

Document - Collection of fields
Analyser  -> stop words, lower/upper case, 


Lucene understands -> String and numeric values like int and float

Search classes :
	IndexSearcher
	Term
	Query
		TermQuery, BooleanQuery, PhraseQuery, TermRangeQuery, NumericRangeQuery
		setBoost()
	TopDocs
	
=====================Chapter 2=====================

A field must be indexed if you want to search on it.
if indexed , a field may also contain term vector
even if not indexed, a field may be stored in index and not used in search.

When we get a document, only stored fields are fetched, indexed and not stored fields are not fetched.

->Schema less design
->multiple entities can be stored in single document
->two same documents can have different schema(one is old and one is new)
->flat structure , denormalized data

=>Indexing Process

raw text taken from source
	|
	|
text is analysed by chain of analysers and converted into stream of tokens
	|
	|
tokens are stored as index


Analysis
	->Analysers
	->Filter
		->stop filter, lowercase filter,stem filter
		

Inverted Index structure
	->tokens are treated as keys for document
	->key=token and value=document
	->which document contain X
	
	
addDocument(Document)
	->takes default analser provided by IndexWriter
addDocument(Document, Analyser)

NOTE : make sure that analyser provided during indexing must match analser during search.

Analyser
	WhiteSpaceAnalyser

Index
	NO
	ANALYZED		--> analyser is used to break the content into tokens
	NOT_ANALYZED	--> analser is not used, content is used as it is, 
						used for those data that should not be broken
	NOT_ANALYZED_NO_NORMS
	ANALYZED_NO_NORMS
	
Store
	NO
	YES
	
TermVector
	NO
	YES
	WITH_POSITIONS
	WITH_OFFSETS
	WITH_POSITIONS_OFFSETS
	

---------------------
	
Term Vector

A term vector is a list of the document's terms and their number of occurrences in that document.
A term is the basic unit searchable in Lucene

Term t = new Term( "field" , "TermText");

when you search Lucene index, you are searching terms
If a field of the document enabled term vector, all terms in that field will be added to document's term vector.

Index
	key --> document id
	
Term Vector 
	term --> (frequency, position, offset)
	
Document Term Vector contains
	->the document id
	->the field name
	->the text of term
	->the frequency
	->position and offsets
	
Applications of Term vector :
-> search result highlighting
-> "related posts" feature in a blog entry

Disadvantage :
->They store a lot of information and often take up a lot of disk space
-> make indexing and searching slower

http://makble.com/what-is-term-vector-in-lucene
http://blog.jpountz.net/post/41301889664/putting-term-vectors-on-a-diet

------------------------

Field options for sorting :

While fetching documents , documents are sorted according to their score by default.
To make these results, sorted by a column , we need to index that column.


multi values fields :

Document doc = new Document();
for (String author : authors) {
	doc.add(new Field("author", author,
	Field.Store.YES,
	Field.Index.ANALYZED));
}

=>Boosting
Boosting may be done during indexing or during searching.

Search-time boosting is more dynamic , but consumes more CPU
 allows you to expose the choice to the user
do not perform much boosting

document.setBoost(1.5F); 
default factor is 1.0 

Field subjectField = new Field("subject", subject, Field.Store.YES, Field.Index.ANALYZED);
subjectField.setBoost(1.2F);


Norms :

All boosts of document and field are combined and then compactly encoded (quantized) 
into a single byte, which is stored per field per document. 
During searching, norms for any field being searched are loaded into memory,
 decoded back into a floating-point number, and
used when computing the relevance score.

Norms for lucene index can be disabled.


=>Indexing Numbers dates and time
(1)
when you want to include numbers as string tokens : WhitespaceAnalyzer and StandardAnalyzer 
when you want to exclude numbers from text while making tokens : SimpleAnalyzer and StopAnalyzer

eg : “Be sure to include Form 1099 in your tax return”

(2)
Numbers where you want precise (equals) matching, range searching, and/or sorting
adding double value to document :
	doc.add(new NumericField("price").setDoubleValue(19.99));

Each numeric value is indexed using a trie structure.
NumericField can also easily handle dates and times by converting them to equivalent ints or longs.

adding timestamp to document :
	doc.add(new NumericField("timestamp").setLongValue(new Date().getTime()));
adding date to document : 
	doc.add(new NumericField("day").setIntValue((int) (new Date().getTime()/24/3600))); 

	
=> Field Truncation :

IndexWriter allows you to truncate per-Field
indexing so that only the first N terms are indexed for an analyzed field.
 text beyond the Nth term is completely ignored

MaxFieldLength.UNLIMITED
MaxFieldLength.LIMITED

=> Directory classes :

SimpleFSDirectory
NIOFSDirectory
MMapDirectory
RAMDirectory
FileSwitchDirectory



=> Concurrency, thread safety, and locking issues

-> accessing an index from multiple JVMs,
-> thread safety of IndexReader and IndexWriter, and 
-> the locking mechanism that Lucene uses to enforce these rules


(1) Any number of read-only IndexReaders may be open at once on a single index.
within a single JVM it’s best for resource utilization 
 and performance reasons to share a single IndexReader instance

(2) Only a single writer may be open on an index at once.
As soon as an IndexWriter is created, a write lock is obtained. 
Only when that IndexWriter is closed is the write lock released

(3) When IndexWriter is open for an index, 
 IndexReader will read document data at the state which was just before taking lock.
 
(4) All threads should share single instance of IndexReader and IndexWriter.
These classes are :
	Thread safe
	Thread friendly
	

=> Buffering and flushing

Documents are initially buffered in memory instead of being immediately written to the disk.
This buffering is done for performance reasons to minimize disk I/O. 
Periodically, these changes are flushed to the index Directory as a new segment.



=====================Chapter 3=====================

 When you’re querying a Lucene index, a TopDocs instance, containing an ordered
array of ScoreDoc, is returned. The array is ordered by score by default

The ScoreDocs themselves aren’t the actual matching documents, but rather references, via
an integer document ID, to the documents matched.


=>programmatically constructing your query

	Term t = new Term("subject", "ant");
	Query query = new TermQuery(t);
	TopDocs docs = searcher.search(query, 10);


=>using QueryParser to translate text entered by the user 

	QueryParser parser = new QueryParser(Version.LUCENE_30, "contents", new SimpleAnalyzer());
	Query query = parser.parse("+JUNIT +ANT -MOCK");
	TopDocs docs = searcher.search(query, 10);


Expression examples that QueryParser handles :

(1)
java			Contain the term java in the default field

(2)
java junit
java OR junit
				Contain the term java or junit, or both, in the default fielda
(3)
+java +junit
java AND junit
				Contain both java and junit in the default field

(4)				
title:ant 		Contain the term ant in the title field

(5)
title:extreme
–subject:sports

title:extreme
AND NOT subject:sports
				Contain extreme in the title field and don’t have sports in the subject field
					
(6)
(agile OR extreme) AND
methodology
				(BooleanQuery) Contain methodology and must also contain agile and/or extreme, all in the default field

(7)
title:"junit in action"
				Contain the exact phrase “junit in action” in the title field
					
(8)					
java* 			(PrefixQuery) Contain terms that begin with java, like javaspaces, javaserver, java.net, and the exact tem java itself.

(9)
java~ 			(FuzzyQuery) Contain terms that are close to the word java, such as lava


----------------

**QueryParser requires an analyzer to break pieces of the query text into terms.
QueryParser is the only searching piece that uses an analyzer, 
other searching APIs do not require analser.
They analyse on the basis of data provided in the API.


Directory dir = FSDirectory.open(new File("/path/to/index"));
IndexReader reader = IndexReader.open(dir);
IndexSearcher searcher = new IndexSearcher(reader);


=> Using explain() to understand hit scoring
	
	TopDocs topDocs = searcher.search(query, 10);
	for (ScoreDoc match : topDocs.scoreDocs) {
		Explanation explanation = searcher.explain(query, match.doc);
	}
	


=> Lucene’s diverse queries

TermQuery,TermRangeQuery,NumericRangeQuery,PrefixQuery,BooleanQuery,PhraseQuery,WildcardQuery,FuzzyQuery

(1)
Term t = new Term("isbn", "9781935182023");
Query query = new TermQuery(t);

(2)
//searching for all books whose title begins with any letter from d to j
TermRangeQuery query = new TermRangeQuery("title2", "d", "j", true, true);

(3)
//  201,003  == march 2010
NumericRangeQuery query = NumericRangeQuery.newIntRange("pubmonth", 200605, 200609, true, true);


(4)
Term term = new Term("category", "/technology/computers/programming"); 
PrefixQuery query = new PrefixQuery(term);


(5)
BooleanQuery
	->BooleanClause.Occur.MUST		-- only that matching documents are considered
	->BooleanClause.Occur.SHOULD	-- term is optional
	->BooleanClause.Occur.MUST_NOT 	--  any documents matching this clause are excluded
	

	TermQuery searchingBooks = new TermQuery(new Term("subject", "search"));
	Query books2010 = NumericRangeQuery.newIntRange("pubmonth", 201001, 201012, true, true);
	
	BooleanQuery searchingBooks2010 = new BooleanQuery();
	searchingBooks2010.add(searchingBooks, BooleanClause.Occur.MUST);
	searchingBooks2010.add(books2010, BooleanClause.Occur.MUST);
	
restricted to a maximum number of clauses : 1,024
	
(6)
PhraseQuery 
 It uses this information to locate documents
  where terms are within a certain distance of one another

The maximum allowable positional distance between terms to be considered a match is called slop. 
Distance is the number of positional moves of terms used to reconstruct the phrase in order.

slop = 0 , quick fox
slop = 1 , quick [irrelevant] fox

By default, a PhraseQuery has its slop factor set to zero.

(7)
WildCardQuery

* for zero or more characters
? for zero or one character


Query query = new WildcardQuery(new Term("contents", "?ild*"));

Performance degradations can occur when you use WildcardQuery

(8) FuzzyQuery
FuzzyQuery matches terms similar to a specified term

Levenshtein distance algorithm , edit distance algorithm

FuzzyQuery uses a threshold rather than a pure edit distance.
The threshold is a factor of the edit distance divided by the string length.


=> QueryParser

(1)
+pubdate:[20100101 TO 20101231] Java AND (Lucene OR Apache)

This query searches for all books about Java that also include Lucene or Apache in their
contents and were published in 2010.

(2)
BooleanQuery query = new BooleanQuery();
query.add(new FuzzyQuery(new Term("field", "kountry")), BooleanClause.Occur.MUST);
query.add(new TermQuery(new Term("title", "western")), BooleanClause.Occur.SHOULD);


"+kountry~0.5 title:western"


(3)
TermRangeQuery

Query query = new QueryParser(Version.LUCENE_30, "subject", analyzer).parse("title2:[Q TO V]");

(4)
BooleanQuery

Boolean Operators :

Verbose syntax	| Shortcut syntax
				|
a AND b			| +a +b
a OR b			| a b
a AND NOT b		| +a –b


By default OR operation is used


(5)
FuzzyQuery

Query query = parser.parse("kountry~");
	 subject:kountry~0.5
	 
query = parser.parse("kountry~0.7");
	 subject:kountry~0.7
	 


	 
=====================Chapter 4=====================

An analyzer tokenizes text by performing any number of operations on it, which could include
	extracting words,
	discarding punctuation, 
	removing accents from characters, 
	lowercasing (also called normalizing), 
	removing common words, 
	reducing words to a root form (stemming), or 
	changing words into the basic form (lemmatization).

This process is also called tokenization, and the chunks of text pulled from a
stream of text are called tokens


Analysis occurs any time text needs to be converted into terms, 
which in Lucene’s core is at two spots: 
->during indexing and 
->when using QueryParser for searching


Analyzing "The quick brown fox jumped over the lazy dog"
 WhitespaceAnalyzer:
 [The] [quick] [brown] [fox] [jumped] [over] [the] [lazy] [dog]
 SimpleAnalyzer:
 [the] [quick] [brown] [fox] [jumped] [over] [the] [lazy] [dog]
 StopAnalyzer:
 [quick] [brown] [fox] [jumped] [over] [lazy] [dog]
 StandardAnalyzer:
 [quick] [brown] [fox] [jumped] [over] [lazy] [dog] 
 
 
 Analyzing "XY&Z Corporation - xyz@example.com"
 WhitespaceAnalyzer:
 [XY&Z] [Corporation] [-] [xyz@example.com]
 SimpleAnalyzer:
 [xy] [z] [corporation] [xyz] [example] [com]
 StopAnalyzer:
 [xy] [z] [corporation] [xyz] [example] [com]
 StandardAnalyzer:
 [xy&z] [corporation] [xyz@example.com]
 
 
(1) WhitespaceAnalyzer
	split text on by whitespace
	doesn’t lowercase each token

(2) SimpleAnalyzer
	splits tokens at nonletter characters
	lowercases each token
	discards numeric characters

(3) StopAnalyzer
	Same as SimpleAnalyzer 
	removes common words (a, an, the)

(4) StandardAnalyzer
	lowercases each token
	removes stop words and punctuation
	identify certain tokens like company name, email, hostname
	

=> What’s inside an analyzer?

A stream of tokens is the fundamental output of the analysis process.
stream of words are divided into stream of characters with start and offset positioning.


TokenStreams: 
	->Tokenizer
	->TokenFilter

A Tokenizer reads characters from a java.io.Reader and creates tokens
A TokenFilter takes tokens in, and produces 
	new tokens by either adding or 
	removing whole tokens or 
	altering the attributes of the incoming tokens
	


TokenStream = Tokenizer + chain of TokenFilter 

Reader -> Tokenizer -> TokenFilter -> TokenFilter -> TokenFilter -> Tokens


--Tokenizer

CharTokenizer
WhitespaceTokenizer
KeywordTokenizer
LetterTokenizer
LowerCaseTokenizer
SinkTokenizer
StandardTokenizer

Sound like quering -- MetaphoneReplacementAnalyzer
synonym - SynonymAnalyser
	
--TokenFilter

LowerCaseFilter
StopFilter			-- removing stop words and punctuation
PorterStemFilter	-- Stemming, country and countries both stem to countri
TeeTokenFilter
ASCIIFoldingFilter
CachingTokenFilter
LengthFilter
StandardFilter



=> Visualizing Analysers :

AnalyzerUtils.displayTokens(analyzer, text);
AnalyzerUtils.displayTokensWithFullDetails(analyzer, text);

