opid count(opid) 	-> mid qid 		GRP opid	(checkAndUpdateMerchantFinalisedQuestion() )

1			-> mid uid qid 				(createUserViewedQuestion() )

qid			-> mid uid				(userViewedQuestions )

qid opid		-> mid uid (opid!=null)			(userAnsweredQuestions )

qid opid count(opid)	-> mid 			GRP qid,opid	(merchant finalised)


==========================


viewed					(2,3)
-> ((mid uid) qid created_at)			


answered				(4)
-> ((mid uid) qid opid created_at)	

// single and multi select answers
// same mid,uid,qid,oid more than once

merchant finalised			(1,5)
-> ((mid) qid opid) count	// valid
-> (mid qid opid) count		// valid
-> ((mid qid) oid) count	// Invalid


==========================

create table user_viewed (mid text, uid int, qid int, created_at timestamp , PRIMARY KEY((mid,uid),qid,created_at) );

insert into user_viewed (mid,uid,qid,created_at) values('1',2,1,toTimestamp(now()) );

//check count of 2 for invalid
select qid,count(*) from user_viewed where mid='1' and uid=1 group by qid;

//check if exists before answer save
select * from user_viewed where mid='1' and uid=1 and qid=1 ;

----------------

-> create table merchant_finalised (mid text, qid int, oid int, count counter , PRIMARY KEY((mid),qid,oid) );

update merchant_finalised SET count = count +1 WHERE mid='2' and qid=1 and oid=2;

select * FROM merchant_finalised WHERE mid='2' and qid=2 ;

select * FROM merchant_finalised WHERE mid='2' ;


-------------------

CREATE TABLE user_answers (mid text, uid int, qid int, oid int ,created_at timestamp, PRIMARY KEY((mid,uid),qid,oid,created_at));

insert into user_answers (mid,uid,qid,oid,created_at) values('1',1,1,1, toUnixTimestamp(now()));
insert into user_answers (mid,uid,qid,oid,created_at) values('1',1,1,1, toUnixTimestamp(now()));
insert into user_answers (mid,uid,qid,oid,created_at) values('1',1,1,2, toUnixTimestamp(now()));

select * from user_answers where mid='1' and uid=1;

==========================

=> Spring Repo
https://www.codingame.com/playgrounds/13642/getting-started-with-spring-data-cassandra
https://github.com/lankydan/spring-data-cassandra/tree/master/src/main


=> Java api 
https://github.com/eugenp/tutorials/tree/master/persistence-modules/java-cassandra


https://stackoverflow.com/questions/24949676/difference-between-partition-key-composite-key-and-clustering-key-in-cassandra
https://teddyma.gitbooks.io/learncassandra/content/model/cql_and_data_structure.html
https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlDatabaseInternalsTOC.html
https://www.guru99.com/cassandra-architecture.html


** imagine Cassandra as a giant HashMap
the Partition keys play the role of that key in map,So each query needs to have them specified. 
clustering keys (columns, which are optional) help in further narrowing your query search 

** One table can have only one combination of same partition key and clustering key

https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlWriteUpdate.html

https://stackoverflow.com/questions/36328063/how-to-return-a-custom-object-from-a-spring-data-jpa-group-by-query

https://stackoverflow.com/questions/44448899/spring-data-for-apache-cassandra-converts-java-time-localdatetime-to-utc

==========================

desc user_viewed;


=========================

<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-cassandra</artifactId>
</dependency>


=========================


@Configuration
public class CassandraConfig extends AbstractCassandraConfiguration {

    @Value("${cassandra.contactpoints}")
    private String contactPoints;

    @Value("${cassandra.port}")
    private int port;

    @Value("${cassandra.keyspace}")
    private String keySpace;

    @Value("${cassandra.basepackages}")
    private String basePackages;

    public SchemaAction getSchemaAction() {
        return SchemaAction.NONE;
    }

    @Override
    protected String getKeyspaceName() {
        return keySpace;
    }

    @Override
    public String[] getEntityBasePackages() {
        return new String[]{basePackages};
    }

    @Bean
    public CassandraClusterFactoryBean cluster() {
        CassandraClusterFactoryBean cluster = new CassandraClusterFactoryBean();
        cluster.setContactPoints(contactPoints);
        cluster.setPort(port);
        return cluster;
    }

    @Bean("cassandraSession")
    public Session cassandraSession() {
        return getRequiredSession();
    }
}


==========================




@Table("user_viewed")
public class UserViewed {

    @PrimaryKeyColumn(name = "contract_id", type = PrimaryKeyType.PARTITIONED, ordinal = 0)
    private String contractId;

    @PrimaryKeyColumn(name = "user_id", type = PrimaryKeyType.PARTITIONED, ordinal = 1)
    private String userId;

    @PrimaryKeyColumn(name = "question_id", type = PrimaryKeyType.CLUSTERED, ordinal = 2)
    private long questionId;

    @PrimaryKeyColumn(name = "created_at", type = PrimaryKeyType.CLUSTERED, ordinal = 3)
    private LocalDateTime createdAt = LocalDateTime.now();

    @Transient
    private long count;

}



==========================

@Repository
public class UserViewedRepoImpl implements UserViewedRepo {

    @Autowired
    private CassandraOperations cassandraTemplate;

    @Autowired
    private Session session;

    @Override
    public List<UserViewed> findAllByMerchantAndUserAndQuestion(String contractId, String userId, long questionId) {
        return cassandraTemplate.select(Query.query(Criteria.where("contractId").is(contractId))
                .and(Criteria.where("userId").is(userId))
                .and(Criteria.where("questionId").is(questionId)), UserViewed.class);
    }

    @Override
    public List<UserViewed> findGroupedQuestionByMerchantAndUser(String contractId, String userId) {
        String query = "Select question_id,count(*) as count from user_viewed " +
                " where contract_id = '"+contractId+"' and user_id = '"+userId+"' group by question_id";
        ResultSet rs = session.execute(query);
        List<UserViewed> userVieweds = new ArrayList<UserViewed>();
        for (Row row : rs) {
            UserViewed data = new UserViewed(row.getLong("question_id"), row.getLong("count"));
            userVieweds.add(data);
        }
        return userVieweds;
    }

    @Override
    public void insert(UserViewed userViewed) {
        cassandraTemplate.insert(userViewed);
    }

    /*cassandraTemplate.update(
            Query.query(Criteria.where("contractId").is(contractId)),
            Update.empty().increment("count"), UserViewed.class);*/
}

==========================

tableX
((col1 col2) col3 col4) col5


select * from tableX where col1='';	//invalid

select * from tableX where col2='';	//invalid

select * from tableX where col1='' and col2='';		//valid


-> Cassandra is a partitioned row store. Rows are organized into tables with a required primary key.
-> persistent multi-dimensional sorted map
-> Data is denormalized, queries should be one per table
-> tables are designed according to our quiries
-> faster for writes, as it is represented as a Map of key value , (update is upsert)
-> stores parts of a data entity or “row” in separate column-families, and has the ability to access these column-families separately. This means that not all parts of a row are picked up in a single I/O operation from storage, which is considered a good thing if only a subset of a row is relevant for a particular query. 

-> data is transparently partitioned across all nodes in a cluster.
   The architecture of Cassandra is “masterless”, meaning all nodes are the same
   if any node in a cluster goes down, one or more copies of that node’s data is available on other machines in the cluster

	
-> data replication
-> AP system in CAP
	choose availability and forgo consistency

Nested Sorted Map data structure

Database	-- Keyspace
Table		-- column family
primary key	-- row key
column name	-- column name/key
column value	-- column value

	Map<RowKey, SortedMap<ColumnKey, ColumnValue>>

A map gives efficient key lookup, and the sorted nature gives efficient scans. 
The number of column keys is unbounded

Partition key and clustering keys


-> For each column family, there are 3 layer of data stores: memtable, commit log and SSTable.

When a write occurs, Cassandra stores the data in a structure in memory, the memtable, 
and also appends writes to the commit log on disk.
The memtable is a write-back cache of data partitions that Cassandra looks up by key.

When memtable contents exceed a configurable threshold, the memtable data, which includes indexes,
 is put in a queue to be flushed to disk. 
To flush the data, Cassandra sorts memtables by partition key and then writes the data to disk sequentially. 


-> All replicas are equally important; there is no primary or master replica

-> Client read or write requests can go to any node in the cluster because all nodes in Cassandra are peers.

When a client connects to a node and issues a read or write request, 
that node serves as the coordinator for that particular client operation.
The job of the coordinator is to act as a proxy between the client application and the nodes (or replicas) 


As long as all replica nodes are up and available, they will get the write regardless of the 
consistency level specified by the client. 
The write consistency level determines how many replica nodes must respond with a success acknowledgment
 in order for the write to be considered successful.
When a node writes and responds, that means it has written to the commit log and puts the mutation into a memtable.

The consistency level defaults to ONE for all write and read operations.


=> Node : 
	Node is the place where data is stored. It is the basic component of Cassandra.

=> Data Center : 
	A collection of nodes are called data center. Many nodes are categorized as a data center.

=> quorum = (sum_of_replication_factors / 2) + 1

=> sum_of_replication_factors = datacenter1_RF + datacenter2_RF + . . . + datacentern_RF

=> LOCAL_QUORUM level is calculated based on the replication factor of the same datacenter as the coordinator node

=> EACH_QUORUM
	every datacenter in the cluster must reach a quorum based on that datacenter's replication factor in order for the write request to succeed.

